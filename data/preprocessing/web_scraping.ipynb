{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f3cd72-7806-4a0e-a29d-1d9c81b34565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import re\n",
    "import os\n",
    "import requests\n",
    "import html2text\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6048f0d3-316d-4333-8c32-44aaab987c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_paginated_pages(start_url, next_link_selector, next_link_attribute='href'):\n",
    "    \"\"\"\n",
    "    Scrapes a website by following a 'next page' link until it's no longer found.\n",
    "\n",
    "    Args:\n",
    "        start_url (str): The URL of the first page to start scraping.\n",
    "        next_link_selector (dict): A dictionary of attributes to find the 'next page' link,\n",
    "                                   e.g., {'class': 'next-button'}.\n",
    "        next_link_attribute (str): The attribute of the link tag that holds the URL,\n",
    "                                   e.g., 'href'.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing all the scraped data from each page.\n",
    "    \"\"\"\n",
    "    current_url = start_url\n",
    "    all_scraped_data = []\n",
    "    page_count = 1\n",
    "\n",
    "    while current_url:\n",
    "\n",
    "        if '_autosummary/nukescripts' in current_url:\n",
    "            print(\"Stopping: Reached a nukescripts autosummary page.\")\n",
    "            break\n",
    "        \n",
    "        print(f\"Scraping page {page_count}: {current_url}\")\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            # Fetch the page content\n",
    "            response = requests.get(current_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            response.raise_for_status() # Check for HTTP errors\n",
    "\n",
    "            # Parse the page with BeautifulSoup\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Find the 'next page' link using the provided selector\n",
    "            next_link_tag = soup.find('a', **next_link_selector)\n",
    "\n",
    "            all_scraped_data.append(current_url)\n",
    "\n",
    "            if next_link_tag:\n",
    "                # Get the URL from the specified attribute\n",
    "                next_page_relative_url = next_link_tag.get(next_link_attribute)\n",
    "                \n",
    "                # Construct the full absolute URL\n",
    "                # The fix is here: use `current_url` instead of `start_url`\n",
    "                current_url = urljoin(current_url, next_page_relative_url)\n",
    "                page_count += 1\n",
    "            else:\n",
    "                print(\"No 'next page' link found. Ending scraping.\")\n",
    "                current_url = None\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching {current_url}: {e}\")\n",
    "            break\n",
    "            \n",
    "    return all_scraped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6272458f-d02e-4f31-ab73-c9f178e34080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your start_url is the first page of the website\n",
    "start_url = 'https://learn.foundry.com/nuke/developers/16.0/pythondevguide/intro.html' \n",
    "# The selector for the next button, based on your screenshot\n",
    "next_link_selector = {'rel': 'next'}\n",
    "\n",
    "# Call the function to begin scraping\n",
    "pages = scrape_paginated_pages(start_url, next_link_selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25026a7a-2601-4d1f-8ee9-b892cd7eff88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_nukescript_pages(start_url, next_link_selector, next_link_attribute='href'):\n",
    "    \"\"\"\n",
    "    Scrapes a website by following a 'next page' link until it's no longer found.\n",
    "\n",
    "    Args:\n",
    "        start_url (str): The URL of the first page to start scraping.\n",
    "        next_link_selector (dict): A dictionary of attributes to find the 'next page' link,\n",
    "                                   e.g., {'class': 'next-button'}.\n",
    "        next_link_attribute (str): The attribute of the link tag that holds the URL,\n",
    "                                   e.g., 'href'.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing all the scraped data from each page.\n",
    "    \"\"\"\n",
    "    current_url = start_url\n",
    "    all_scraped_data = []\n",
    "    page_count = 1\n",
    "\n",
    "    while current_url:\n",
    "\n",
    "        if '_autosummary/nukescripts' not in current_url:\n",
    "            print(\"Stopping: Reached a nukescripts autosummary page.\")\n",
    "            break\n",
    "            \n",
    "        if '_autosummary/nukescripts.autoBackdrop.html' in current_url:\n",
    "            current_url = 'https://learn.foundry.com/nuke/developers/16.0/pythondevguide/_autosummary/nukescripts.anySelectedVertexInfo.html'\n",
    "        \n",
    "        print(f\"Scraping page {page_count}: {current_url}\")\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            # Fetch the page content\n",
    "            response = requests.get(current_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            response.raise_for_status() # Check for HTTP errors\n",
    "\n",
    "            # Parse the page with BeautifulSoup\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Find the 'next page' link using the provided selector\n",
    "            next_link_tag = soup.find('a', **next_link_selector)\n",
    "\n",
    "            all_scraped_data.append(current_url)\n",
    "\n",
    "            if next_link_tag:\n",
    "                # Get the URL from the specified attribute\n",
    "                next_page_relative_url = next_link_tag.get(next_link_attribute)\n",
    "                \n",
    "                # Construct the full absolute URL\n",
    "                # The fix is here: use `current_url` instead of `start_url`\n",
    "                current_url = urljoin(current_url, next_page_relative_url)\n",
    "                page_count += 1\n",
    "            else:\n",
    "                print(\"No 'next page' link found. Ending scraping.\")\n",
    "                current_url = None\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching {current_url}: {e}\")\n",
    "            break\n",
    "            \n",
    "    return all_scraped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cfbfbe-6019-4c37-9cd7-648739bff44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your start_url is the first page of the website\n",
    "start_url = 'https://learn.foundry.com/nuke/developers/16.0/pythondevguide/_autosummary/nukescripts.widgetgroup.Qt.html' \n",
    "# The selector for the next button, based on your screenshot\n",
    "prev_link_selector = {'rel': 'prev'}\n",
    "\n",
    "# Call the function to begin scraping\n",
    "nukescript_pages = scrape_nukescript_pages(start_url, prev_link_selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa719c0-9a60-4aba-80d8-32a4bae47902",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(nukescript_pages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2d483a-1a0c-43ba-8e6c-df58916acf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_response(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching the URL: {e}\")\n",
    "        return\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00323023-e4b8-4323-bf40-27e3c9e8293c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_data(urls, output):\n",
    "    \"\"\"\n",
    "    Scrapes a URL, specifically finds 'highlight' classes for code,\n",
    "    formats them as Markdown code blocks, and saves the result.\n",
    "    \"\"\"\n",
    "\n",
    "    for url in urls:\n",
    "\n",
    "        response = get_page_response(url)\n",
    "    \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Isolate the main content area to avoid navbars, footers, etc.\n",
    "        main_content = soup.find('main')\n",
    "        if not main_content:\n",
    "            main_content = soup.body\n",
    "\n",
    "        # Find all divs with the 'highlight' class within the main content\n",
    "        code_blocks = main_content.find_all(class_='highlight')\n",
    "        \n",
    "        print(f\"Found {len(code_blocks)} code blocks to format.\")\n",
    "\n",
    "        for block in code_blocks:\n",
    "            # Extract the raw text from the code block\n",
    "            code_text = block.get_text()\n",
    "    \n",
    "            language = 'python'\n",
    "            \n",
    "            # Create the formatted Markdown code block as a string\n",
    "            markdown_code_block = f\"```{language}\\n{code_text.strip()}\\n```\"\n",
    "            \n",
    "            # Create a new BeautifulSoup tag (<pre>) and replace the original\n",
    "            # 'highlight' div with this new tag containing our Markdown text.\n",
    "            new_tag = soup.new_tag(\"pre\")\n",
    "            new_tag.string = markdown_code_block\n",
    "            block.replace_with(new_tag)\n",
    "    \n",
    "        # Now, convert the MODIFIED main_content to Markdown.\n",
    "        # html2text will respect the content of the <pre> tags we just made.\n",
    "        h = html2text.HTML2Text()\n",
    "        h.body_width = 0\n",
    "        markdown_output = h.handle(str(main_content))\n",
    "\n",
    "        # Create a filename from the URL\n",
    "        raw_filename = url.split(\"/\")[-1].replace(\".html\", \".md\")\n",
    "        if '#' in raw_filename:\n",
    "            raw_filename = raw_filename.split('#')[0]\n",
    "            \n",
    "        base_name, extension = os.path.splitext(raw_filename)\n",
    "        \n",
    "        if '.' in base_name:\n",
    "            base_name = base_name.replace('.', '_')\n",
    "        filename = base_name + extension\n",
    "        output_path = f\"{output}/{filename}\"\n",
    "            \n",
    "        # Save the Markdown content to a file\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(markdown_output)\n",
    "\n",
    "        print(f\"Successfully saved {url} to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc46cb8-ef76-4fd8-8837-195630fcf733",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = os.path.join(os.getcwd(), '..', 'documents', 'raw')\n",
    "# print(output_dir)\n",
    "\n",
    "scrape_data(nukescript_pages, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57687322-14d3-40b0-bd48-dde704acc974",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
